

DataFlow=> serverless Apache beam pipelines! 

2 kinds of data points: Bounded (batch) and Unbounded (stream)

Windows:
1) Stream are TIME based windows-> Fixed and SLIDING windows
2) Batches are EVENT based windows-> Session windows 

2 types of behaviors in event windows:
1) Accumulating
2) Discarding 


AI-> Mehcanical, electrical, chemical, electronic, genetic

-> Machine Learning 
	-> graph theory based (boundary/algorithm=> y=mx+c) 
	-> pattern recognition (neural networks=> y' = m'x' + c')
		- deep learning 
		- regex (porter stemming)
y = mx + c
inference = weights * inputs + bias 

Linear Regression, KNN, K-means
Discrete and Combinatorial Math-> Graph Theory
	Continous-> audio signal
	discrete -> city population


-> weather prediction
	-> temp-> yester-> 23, day_before->22, before->21, 22, 23
	21, 22, ? , 22, 21, 22, 23..... 
	-> missing data = prev + next/2
			= mean , median, any other formula
		=> NONE OF THEM WAS WRONG!!!! 
	=> if statistics makes sense=> ML with Graphs

-> classrom-> 40 students-> 20 red color, 10 blue, 5 yellow, 5 purple
	-> a new student enter classroom-> color = ?
	-> red + blue/2 = green?
	-> If above doesn't make sense, neither will it's graph!!!
	-> new classes may be created that didn't exist!!!

	-> we move to Deep Learning instead of Graphs!
	-> We break these variables into smaller units/patters
		-> if there was NO patter on the smaller UNIT
		-> THEN there was NO pattern on the original data!!

Blue-> (r,g,b) -> MATRIX representing pixels of sky, every cell
	could again be a tuple!!! 
		


-> There was a king in land far away
	-> hired data sc. -> happiness index of the kingdom

	-> surveys, Q&A -> inference-> take steps 1,2,3 else
	there will be a riot!!
	-> king implemented 1,2,3-> 6 months later-> NO RIOT!!

-> Do we reward DS for correct prediction, or hang them for false 
	inference?
	-> ML is PUREST form of GUESS
	-> IT IS NOT MEANT TO BE ACCURATE
	-> if there is high sense of precision, there is SOMETHING
	WRONG in algo!!!!

emotion recognition:
	y = mx + c
	emotion = weights * p_dist(happiness, sorrow, disgust...) + 
		bias 
final_emotion = SUM( p(fn(individual_emotions) ))= 1


Ext. Links:
https://mrbubu.pro/how-to-stop-wasting-money-on-google-bigquery-in-google-data-studio/
datacamp, pluralsight, coursera -> python, sql, R

Fill in the feedback please-> plz let me know via chat window
	once done (link on chat window)

Composer-> we will need to create CLUSTER beforehand as it takes
15-20 mins


Queries and sub-query-> no idea 

Big query-> no actual, pointer manipulation
	=> re-ordering the pointers-> across entire table/partition 
	=> ORDER inside, re-order might again happen!!!
	=> keep on the OUTERMOST query

Tradeoff-> performance and availability 
	d(per) + d(ava) = 1

VPN-> internet is expensive!!! 
	-> make-> router, gateway, server, isp, country, cont, 
SMTP/ICMP/multicast-> native cloud email providers

THings data engineers should worry about: (outside DE and DS)
	-> DevOps (Docker , Kubernetes, CI/CD)
	-> Security (IAM, RBAC, encry/hashing, roles & permissions,
		basic sql injections, denial-of-services, botnet, 
		data corruption, integrity, DLP(PII/SPDI), 
	compliance(GDPR,ISO27001, PCI, HIPPA...} 
	-> infra-> clustering (managed) v/s serverless,
		scalability, high-availability & fault tolerance,
		failovers, geo/regional redundancies or replications
	-> performance & cost-> use calculator, predict what queries/
	fns/UDF are more expensive than others

Exam-> MLOps -> KubeFlow, Pipelines (batch, realtime), 
	inference (production), dev/test-> non-enterprise 
	- App engine, vm flask API, kubernetes clusters 	 
	- Edge-> except for monitoring, security
		-> telemetry-> async
		


n-high CPU-4 core
 128 slots -> each_machine X no._of_machines 

scale->horizontal
CLUSTER> vertically-> 

CREATE a new cluster-> then shift your compute load 


identify whether you need RAM, CPU or disk 
	- High CPU machine for Composer 
	- Apache Spark -> better RAM or disk (if in IN_DISK mode)


Spark-> replication = 3, IN-mem -> more RAM , minimal disk consider

process 10GB of data at a time-> in spark-> data X replication
		=> data X 3 = 3x

RAM= ? 10X 3 = 30GB + 10GB ( => to process 10GB of data in ONE-go,
 i need 40 GB of ram


ML automation is basically 3 activities:
	1) Create a model with features EXTRACTed (y=mx+c)
	2) Evaluate the model performance (comparing this y with
		actual known results)
	3) Predict the future values (if model is ok, use it)
BigQuery-> 3 methods to do it 

eigenvalue, eigenvector

f(x) = all calc values y for every x => y = mx +c, y1 for x1, yn for nx!
Model-> f(x), Label-> y1,y2...yn, input=> x1,x2...xn 
weights -> m1,m2,..mn for x1,x2..xn respectively
 ML.training_info <- select * from gives training progress!
 ML.weights for weights post training  
bias -> c

2 kind of orchestration products:
	- Composer (for all GCP activities)
	- Kubeflow (AI or ML workloads) 

2 kinds of Registeries:
1) Container Registery -> docker and kubernetes
2) key registery -> service sercrets (connection string storage for
	an API connecting to db) 



Pandas -> Panel Dataframes 
Data set is total data, windows (or panels) of data from dataset
are called DataFrames (size(dataFrame) <= size(dataset))

extra info out of course:
ML examples in repo:
https://github.com/a-forty-two/cylons/blob/master/02_BreastCancerAnalysis.ipynb